# Introduction

The **Quantifying Metacognition in LLMs** project explores how large language models exhibit self-awareness and reasoning under uncertainty using a metacognitive framework.  
This page introduces the motivation and theoretical background for our work, inspired by the dual-process cognitive model and recent advances in LLM ensembles.

## Motivation

Understanding *how* models think about their own reasoning processes provides insight into interpretability, reliability, and generalization.  
Our goal is to develop metrics and representations that quantify metacognitive awareness in artificial systems.

## Core Idea

We build on prior work in cognitive science and AI to model the transition between **System 1 (intuitive reasoning)** and **System 2 (reflective reasoning)** processes within ensembles of LLMs.
